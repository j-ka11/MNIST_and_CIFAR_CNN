{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our MNIST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from torchvision import transforms\n",
    "\n",
    "trans = torchvision.transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='data\\mnist', train=True, transform=trans, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='data\\mnist', train=False, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
    "\n",
    "torch.save(model.state_dict(), 'saved_models/mnist_conv_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "\n",
    "p = figure(y_axis_label='Loss', width=850, y_range=(0, 1), title='PyTorch ConvNet results')\n",
    "p.extra_y_ranges = {'Accuracy': Range1d(start=0, end=100)}\n",
    "p.add_layout(LinearAxis(y_range_name='Accuracy', axis_label='Accuracy (%)'), 'right')\n",
    "p.line(np.arange(len(loss_list)), loss_list)\n",
    "p.line(np.arange(len(loss_list)), np.array(acc_list) * 100, y_range_name='Accuracy', color='red')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our CIFAR-10 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='data\\cifar', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='data\\cifar', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CIFAR_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CIFAR_CNN, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "\n",
    "# This training loop optimizes batch size\n",
    "cifar_batch_models = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "list_of_training_losses = []\n",
    "list_of_testing_losses = []\n",
    "for batch in range(1, 10):\n",
    "    next_batch = 2 ** batch\n",
    "    print()\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('Training with batch size {}'.format(next_batch))\n",
    "    \n",
    "    cifar_model_batch = CIFAR_CNN()\n",
    "    cifar_model_batch.to(device)\n",
    "    cifar_batch_models.append(cifar_model_batch)\n",
    "    \n",
    "    optimizer = optim.SGD(cifar_model_batch.parameters(), lr=0.01)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=next_batch, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=next_batch, shuffle=False, num_workers=2)\n",
    "    \n",
    "    training_losses = []\n",
    "    testing_losses = []\n",
    "    for epoch in range(15):\n",
    "        # Train\n",
    "        start = time.time()\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()                 # Zero the gradients\n",
    "\n",
    "            outputs = cifar_model_batch(inputs)                 # Forward pass\n",
    "            loss = criterion(outputs, targets)    # Compute the Loss\n",
    "            loss.backward()                       # Compute the Gradients\n",
    "\n",
    "            optimizer.step()                      # Updated the weights\n",
    "            training_losses.append(loss.item())\n",
    "            end = time.time()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Batch Index : %d Loss : %.3f Time : %.3f seconds ' % (batch_idx, np.mean(training_losses), end - start))\n",
    "\n",
    "                start = time.time()\n",
    "                \n",
    "        # Evaluate\n",
    "        cifar_model_batch.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                outputs = cifar_model_batch(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                testing_losses.append(loss.item())\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "            print('Epoch : %d Test Acc : %.3f' % (epoch, 100.*correct/total))\n",
    "            print('--------------------------------------------------------------')\n",
    "                \n",
    "    list_of_training_losses.append(training_losses)\n",
    "    list_of_testing_losses.append(testing_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Here we graph training losses and testing losses for the different batch sizes\n",
    "num_batches = len(list_of_training_losses)\n",
    "fig, axes = plt.subplots(num_batches, 2, figsize=(15,40))\n",
    "for batch_idx in range(num_batches):\n",
    "    training_ax = axes[batch_idx][0]\n",
    "    testing_ax = axes[batch_idx][1]\n",
    "    \n",
    "    training_ax.plot(list_of_training_losses[batch_idx])\n",
    "    training_ax.set_title('Training loss for a batch size of {}'.format(2 ** (batch_idx + 1)))\n",
    "    testing_ax.plot(list_of_testing_losses[batch_idx])\n",
    "    testing_ax.set_title('Testing loss for a batch size of {}'.format(2 ** (batch_idx + 1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "\n",
    "best_batch_size_low_epoch = 16\n",
    "best_batch_size_high_epoch = 512\n",
    "\n",
    "trainloader_lr = torch.utils.data.DataLoader(trainset, batch_size=best_batch_size_low_epoch, shuffle=True, num_workers=2)\n",
    "testloader_lr = torch.utils.data.DataLoader(testset, batch_size=best_batch_size_low_epoch, shuffle=False, num_workers=2)\n",
    "\n",
    "# This training loop optimizes learning rate\n",
    "cifar_lr_models = []\n",
    "\n",
    "criterion_lr = nn.CrossEntropyLoss()\n",
    "lrs = [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "\n",
    "list_of_training_losses_lr = []\n",
    "list_of_testing_losses_lr = []\n",
    "for learning_rate in lrs:\n",
    "    print()\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('Training with learning rate {}'.format(learning_rate))\n",
    "    \n",
    "    cifar_model_lr = CIFAR_CNN()\n",
    "    cifar_model_lr.to(device)\n",
    "    cifar_lr_models.append(cifar_model_lr)\n",
    "    \n",
    "    optimizer_lr = optim.SGD(cifar_model_lr.parameters(), lr=learning_rate)\n",
    "    \n",
    "    training_losses_lr = []\n",
    "    testing_losses_lr = []\n",
    "    for epoch in range(15):\n",
    "        # Train\n",
    "        start = time.time()\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader_lr):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer_lr.zero_grad()                 # Zero the gradients\n",
    "\n",
    "            outputs = cifar_model_lr(inputs)      # Forward pass\n",
    "            loss = criterion_lr(outputs, targets) # Compute the Loss\n",
    "            loss.backward()                       # Compute the Gradients\n",
    "\n",
    "            optimizer_lr.step()                   # Updated the weights\n",
    "            training_losses_lr.append(loss.item())\n",
    "            end = time.time()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Batch Index : %d Loss : %.3f Time : %.3f seconds ' % (batch_idx, np.mean(training_losses_lr), end - start))\n",
    "\n",
    "                start = time.time()\n",
    "                \n",
    "        # Evaluate\n",
    "        cifar_model_lr.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader_lr):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                outputs = cifar_model_lr(inputs)\n",
    "                loss = criterion_lr(outputs, targets)\n",
    "                testing_losses_lr.append(loss.item())\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "            print('Epoch : %d Test Acc : %.3f' % (epoch, 100.*correct/total))\n",
    "            print('--------------------------------------------------------------')\n",
    "                \n",
    "    list_of_training_losses_lr.append(training_losses_lr)\n",
    "    list_of_testing_losses_lr.append(testing_losses_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('variables/learning_rate.pickle', 'wb') as f:\n",
    "    pickle.dump([list_of_training_losses_lr, list_of_testing_losses_lr], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(fig)\n",
    "\n",
    "# Here we graph training losses and testing losses for the different learning rates\n",
    "num_lrs = len(lrs)\n",
    "fig_lr, axes_lr = plt.subplots(num_lrs, 2, figsize=(15,40))\n",
    "for lr_idx in range(num_lrs):\n",
    "    training_ax_lr = axes_lr[lr_idx][0]\n",
    "    testing_ax_lr = axes_lr[lr_idx][1]\n",
    "    \n",
    "    training_ax_lr.plot(list_of_training_losses_lr[lr_idx])\n",
    "    training_ax_lr.set_title('Training loss for a learning rate of {}'.format(lrs[lr_idx]))\n",
    "    testing_ax_lr.plot(list_of_testing_losses_lr[lr_idx])\n",
    "    testing_ax_lr.set_title('Testing loss for a learning rate of {}'.format(lrs[lr_idx]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_learning_rate = 0.01\n",
    "\n",
    "trainloader_mom = torch.utils.data.DataLoader(trainset, batch_size=best_batch_size_low_epoch, shuffle=True, num_workers=2)\n",
    "testloader_mom = torch.utils.data.DataLoader(testset, batch_size=best_batch_size_low_epoch, shuffle=False, num_workers=2)\n",
    "\n",
    "# This training loop optimizes momentum\n",
    "cifar_mom_models = []\n",
    "\n",
    "criterion_mom = nn.CrossEntropyLoss()\n",
    "momentums = [0.0, 0.2, 0.4, 0.6, 0.8, 0.99]\n",
    "\n",
    "list_of_training_losses_mom = []\n",
    "list_of_testing_losses_mom = []\n",
    "for mom in momentums:\n",
    "    print()\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('Training with momentum {}'.format(mom))\n",
    "    \n",
    "    cifar_model_mom = CIFAR_CNN()\n",
    "    cifar_model_mom.to(device)\n",
    "    cifar_mom_models.append(cifar_model_mom)\n",
    "    \n",
    "    optimizer_mom = optim.SGD(cifar_model_mom.parameters(), lr=best_learning_rate, momentum=mom)\n",
    "    \n",
    "    training_losses_mom = []\n",
    "    testing_losses_mom = []\n",
    "    for epoch in range(15):\n",
    "        # Train\n",
    "        start = time.time()\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader_mom):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer_mom.zero_grad()                 # Zero the gradients\n",
    "\n",
    "            outputs = cifar_model_mom(inputs)      # Forward pass\n",
    "            loss = criterion_mom(outputs, targets) # Compute the Loss\n",
    "            loss.backward()                       # Compute the Gradients\n",
    "\n",
    "            optimizer_mom.step()                   # Updated the weights\n",
    "            training_losses_mom.append(loss.item())\n",
    "            end = time.time()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Batch Index : %d Loss : %.3f Time : %.3f seconds ' % (batch_idx, np.mean(training_losses_mom), end - start))\n",
    "\n",
    "                start = time.time()\n",
    "                \n",
    "        # Evaluate\n",
    "        cifar_model_mom.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader_mom):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                outputs = cifar_model_mom(inputs)\n",
    "                loss = criterion_mom(outputs, targets)\n",
    "                testing_losses_mom.append(loss.item())\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "            print('Epoch : %d Test Acc : %.3f' % (epoch, 100.*correct/total))\n",
    "            print('--------------------------------------------------------------')\n",
    "                \n",
    "    list_of_training_losses_mom.append(training_losses_mom)\n",
    "    list_of_testing_losses_mom.append(testing_losses_mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(fig_lr)\n",
    "\n",
    "# Here we graph training losses and testing losses for the momentum values\n",
    "num_moms = len(momentums)\n",
    "fig_mom, axes_mom = plt.subplots(num_moms, 2, figsize=(15,40))\n",
    "for mom_idx in range(num_moms):\n",
    "    training_ax_mom = axes_mom[mom_idx][0]\n",
    "    testing_ax_mom = axes_mom[mom_idx][1]\n",
    "    \n",
    "    training_ax_mom.plot(list_of_training_losses_mom[mom_idx])\n",
    "    training_ax_mom.set_title('Training loss for a momentum of {}'.format(momentums[mom_idx]))\n",
    "    testing_ax_mom.plot(list_of_testing_losses_mom[mom_idx])\n",
    "    testing_ax_mom.set_title('Testing loss for a momentum of {}'.format(momentums[mom_idx]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "\n",
    "best_batch_size_low_epoch = 16\n",
    "best_batch_size_high_epoch = 512\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=best_batch_size_high_epoch, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=best_batch_size_high_epoch, shuffle=False, num_workers=2)\n",
    "\n",
    "# This training loop uses the best hyperparameters\n",
    "\n",
    "best_learning_rate = 0.01\n",
    "best_momentum = 0.3\n",
    "\n",
    "cifar_model = CIFAR_CNN()\n",
    "cifar_model.load_state_dict(torch.load('saved_models/cifar_conv_model.ckpt'))\n",
    "cifar_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cifar_model.parameters(), lr=best_learning_rate, momentum=best_momentum)\n",
    "\n",
    "training_losses = []\n",
    "testing_losses = []\n",
    "for epoch in range(30):\n",
    "    # Train\n",
    "    start = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()                 # Zero the gradients\n",
    "\n",
    "        outputs = cifar_model(inputs)         # Forward pass\n",
    "        loss = criterion(outputs, targets)    # Compute the Loss\n",
    "        loss.backward()                       # Compute the Gradients\n",
    "\n",
    "        optimizer.step()                      # Updated the weights\n",
    "        training_losses.append(loss.item())\n",
    "        end = time.time()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Batch Index : %d Loss : %.3f Time : %.3f seconds ' % (batch_idx, np.mean(training_losses), end - start))\n",
    "      \n",
    "            start = time.time()\n",
    "        \n",
    "    # Evaluate\n",
    "    cifar_model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = cifar_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            testing_losses.append(loss.item())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        print('Epoch : %d Test Acc : %.3f' % (epoch, 100.*correct/total))\n",
    "        print('--------------------------------------------------------------')\n",
    "    cifar_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cifar_model.state_dict(), 'saved_models/cifar_conv_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
